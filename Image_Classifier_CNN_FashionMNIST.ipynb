{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Classifier_CNN_FashionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5tDd/zOPpQHOgIgp0aXUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toluwee/Computer-Vision-Project/blob/master/Image_Classifier_CNN_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO7Eo4oBt5lb"
      },
      "source": [
        "# Classifying Fashion MNIST Dataset with CNN Deep Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlMGoZB0DqKq"
      },
      "source": [
        "## Project Overview\n",
        "\n",
        "I created an image classification application using a deep neural network with convolutions. This application trains a deep learning model on a dataset of images. It then uses the trained model to classify new images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICGXVXDmnoT-"
      },
      "source": [
        "## Key Skills Demonstrated:\n",
        "\n",
        "* Tensorflow and convolutional neural networks\n",
        "* Model validation and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGrOtBu4Yn6G"
      },
      "source": [
        "## About dataset\n",
        "\n",
        "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "\n",
        "More information about the dataset available [here](https://github.com/zalandoresearch/fashion-mnist)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmKRHu2Yvhy"
      },
      "source": [
        "## Objective \n",
        "\n",
        "To create a model that correctly classifies images of fashion items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYPqwFwgfZ0S"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Necessary libraries are imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSnWTZTGW0HT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpzhtNMaVJC"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuvZkZLocSrA"
      },
      "source": [
        "#  Import the fashion MNIST dataset from keras dataset into randomly pre-shuffled training and test data\n",
        "(X_train,y_train), (X_test, y_test) = tf.keras.fashion_mnist.load_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz_V3jl_fnAk"
      },
      "source": [
        "### Explore Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZj3X81HbYz9",
        "outputId": "96539a91-1449-400e-9971-089debf4bb61"
      },
      "source": [
        "# Shape of training set\n",
        "print (X_train.shape)\n",
        "\n",
        "# Confirm data type\n",
        "print(X_train.dtype)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "lzpGxUKzPGmu",
        "outputId": "c7d42f11-5cae-4354-89a7-75ba338b986a"
      },
      "source": [
        "\n",
        "# View sample image\n",
        "plt.imshow(X_train[100],cmap='gray')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0ab11eda90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZZ0sAU2b5Gs"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp8p70GJcZog"
      },
      "source": [
        "# reshape dataset dimensions to have a single channel\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
        " \n",
        "# Normalize the pixel values of the images to the 0-1 range from 0-255 range by dividing by 255.0\n",
        "X_train = X_train/ 255.0\n",
        "X_test = X_test/ 255.0\n",
        "\n",
        "# Seperate validation set from training dataset\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh52a_x1i3FM"
      },
      "source": [
        "# from f.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     # zoom_range=[0.5,1.0],\n",
        "#                                   #  brightness_range=[0.2,1.0],\n",
        "#                                   #  rotation_range=90,\n",
        "#                                   #  horizontal_flip=True,\n",
        "#                                   #  height_shift_range=0.5,\n",
        "#                                   #  width_shift_range=[-200,200],                                   \n",
        "#                                    )\n",
        "# validation_datagen = ImageDataGenerator(\n",
        "#     # zoom_range=[0.5,1.0],\n",
        "#     #                                brightness_range=[0.2,1.0],\n",
        "#     #                                rotation_range=90,\n",
        "#                                   #  horizontal_flip=True,\n",
        "#                                   #  height_shift_range=0.5,\n",
        "#                                   #  width_shift_range=[-200,200],\n",
        "#                                    )\n",
        "# test_datagen = ImageDataGenerator(\n",
        "#     # zoom_range=[0.5,1.0],\n",
        "#                                   #  brightness_range=[0.2,1.0],\n",
        "#                                   #  rotation_range=90,\n",
        "#                                   #  horizontal_flip=True,\n",
        "#                                   #  height_shift_range=0.5,\n",
        "#                                   #  width_shift_range=[-200,200],\n",
        "#                                    )\n",
        "\n",
        "# # Flow training images in batches of 128 using train_datagen generator\n",
        "\n",
        "# train_gen = train_datagen.flow(x=X_train, \n",
        "#                                       y=y_train,\n",
        "#                                       # target_size=(28,28),  # All images will be resized to 28x 28\n",
        "#                                       batch_size=32,\n",
        "#                                       )\n",
        "\n",
        "# valid_gen = validation_datagen.flow(x=X_valid, \n",
        "#                                       y=y_valid,\n",
        "#                                       # target_size=(28,28),  # All images will be resized to 28x 28\n",
        "#                                       batch_size=32,\n",
        "#                                       )\n",
        "\n",
        "\n",
        "# test_gen = test_datagen.flow(x=X_test, \n",
        "#                                       y=y_test,\n",
        "#                                       # target_size=(28,28),  # All images will be resized to 28x 28\n",
        "#                                       batch_size=32,\n",
        "#                                       )\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaado79sf4JJ"
      },
      "source": [
        "### Assign class names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUs80zHLfAOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f171d5-9be9-4e90-d4bd-b6d555d06a00"
      },
      "source": [
        "#  Define the class labels\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "#  Get a sample label with \n",
        "print(class_names[y_train[100]])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pullover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwEQ117KfVQ7"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "Keras will be used to build and implement the model.\n",
        "To define the model: \n",
        "*  Convolutional layers are added \n",
        "* Final result flattened to feed into the densely connected layers.\n",
        "* Densely connected layers added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rhBg7vGbuhd"
      },
      "source": [
        "#  Refresh tensorflow environment \n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hccjae63-vLP"
      },
      "source": [
        "# This callback will stop the training when there is no improvement in  \n",
        "# the validation loss for three consecutive epochs.\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jcSrzU-fUyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00836b9b-cb8e-44a9-8c46-40726760ad02"
      },
      "source": [
        "# Model definition\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    \n",
        "    # This is the first convolution layer to slightly reduce the size of the feature maps \n",
        "    tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", activation='relu', input_shape=(28,28,1)),\n",
        "    \n",
        "    # Pooling layer halves the dimension\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Randomly applies dropouts that sets input units to 0 with a frequency of rate 0.3 to prevent overfitting.\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    # The second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding=\"same\", activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    # The third convolution layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    # Flatten the results \n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # densely connected hidden layer with 512 neuron\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        " \n",
        "    # Contains 10 output neuron. It will contain a value from 0-9 where 0 for \"T-shirt/top\" and 9 for \"Ankle boot\"\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "#  Description of model\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               295424    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 323,850\n",
            "Trainable params: 323,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQcPzRTqLqgh"
      },
      "source": [
        "# Actual building of the model\n",
        "model.compile(optimizer= 'nadam',\n",
        "              loss= 'sparse_categorical_crossentropy',\n",
        "              metrics= ['accuracy']) # accuracy is measured cos it is classification"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGsxfeniNskG"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nSEDXjtNucU",
        "outputId": "dda564f5-a8e5-44c8-df11-e84ee9903376"
      },
      "source": [
        "history = model.fit(\n",
        "                    x= X_train, \n",
        "                    y= y_train, \n",
        "                    # train_gen,\n",
        "                    epochs= 50, \n",
        "                    verbose = 1,\n",
        "                    validation_data =(X_valid, y_valid),\n",
        "                    callbacks = [callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1719/1719 [==============================] - 63s 36ms/step - loss: 0.8101 - accuracy: 0.6949 - val_loss: 0.3706 - val_accuracy: 0.8648\n",
            "Epoch 2/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.4212 - accuracy: 0.8408 - val_loss: 0.3032 - val_accuracy: 0.8876\n",
            "Epoch 3/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.3648 - accuracy: 0.8654 - val_loss: 0.2896 - val_accuracy: 0.8918\n",
            "Epoch 4/50\n",
            "1719/1719 [==============================] - 61s 36ms/step - loss: 0.3288 - accuracy: 0.8744 - val_loss: 0.2619 - val_accuracy: 0.9002\n",
            "Epoch 5/50\n",
            "1719/1719 [==============================] - 63s 36ms/step - loss: 0.3158 - accuracy: 0.8832 - val_loss: 0.2496 - val_accuracy: 0.9046\n",
            "Epoch 6/50\n",
            "1719/1719 [==============================] - 63s 37ms/step - loss: 0.2927 - accuracy: 0.8906 - val_loss: 0.2441 - val_accuracy: 0.9064\n",
            "Epoch 7/50\n",
            "1719/1719 [==============================] - 63s 36ms/step - loss: 0.2871 - accuracy: 0.8928 - val_loss: 0.2364 - val_accuracy: 0.9144\n",
            "Epoch 8/50\n",
            "1719/1719 [==============================] - 61s 36ms/step - loss: 0.2711 - accuracy: 0.8983 - val_loss: 0.2359 - val_accuracy: 0.9136\n",
            "Epoch 9/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.2743 - accuracy: 0.8979 - val_loss: 0.2279 - val_accuracy: 0.9150\n",
            "Epoch 10/50\n",
            "1719/1719 [==============================] - 63s 37ms/step - loss: 0.2615 - accuracy: 0.9003 - val_loss: 0.2184 - val_accuracy: 0.9188\n",
            "Epoch 11/50\n",
            "1719/1719 [==============================] - 63s 36ms/step - loss: 0.2570 - accuracy: 0.9010 - val_loss: 0.2212 - val_accuracy: 0.9144\n",
            "Epoch 12/50\n",
            "1719/1719 [==============================] - 61s 36ms/step - loss: 0.2539 - accuracy: 0.9040 - val_loss: 0.2170 - val_accuracy: 0.9196\n",
            "Epoch 13/50\n",
            "1719/1719 [==============================] - 61s 35ms/step - loss: 0.2459 - accuracy: 0.9071 - val_loss: 0.2232 - val_accuracy: 0.9150\n",
            "Epoch 14/50\n",
            "1719/1719 [==============================] - 59s 34ms/step - loss: 0.2501 - accuracy: 0.9035 - val_loss: 0.2147 - val_accuracy: 0.9212\n",
            "Epoch 15/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2463 - accuracy: 0.9062 - val_loss: 0.2187 - val_accuracy: 0.9164\n",
            "Epoch 16/50\n",
            "1719/1719 [==============================] - 59s 34ms/step - loss: 0.2352 - accuracy: 0.9120 - val_loss: 0.2207 - val_accuracy: 0.9188\n",
            "Epoch 17/50\n",
            "1719/1719 [==============================] - 59s 35ms/step - loss: 0.2363 - accuracy: 0.9104 - val_loss: 0.2038 - val_accuracy: 0.9252\n",
            "Epoch 18/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2363 - accuracy: 0.9109 - val_loss: 0.2113 - val_accuracy: 0.9192\n",
            "Epoch 19/50\n",
            "1719/1719 [==============================] - 59s 34ms/step - loss: 0.2371 - accuracy: 0.9118 - val_loss: 0.2151 - val_accuracy: 0.9194\n",
            "Epoch 20/50\n",
            "1719/1719 [==============================] - 59s 34ms/step - loss: 0.2338 - accuracy: 0.9130 - val_loss: 0.2137 - val_accuracy: 0.9216\n",
            "Epoch 21/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2348 - accuracy: 0.9115 - val_loss: 0.2083 - val_accuracy: 0.9264\n",
            "Epoch 22/50\n",
            "1719/1719 [==============================] - 61s 35ms/step - loss: 0.2329 - accuracy: 0.9107 - val_loss: 0.2077 - val_accuracy: 0.9250\n",
            "Epoch 23/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.2305 - accuracy: 0.9118 - val_loss: 0.2135 - val_accuracy: 0.9224\n",
            "Epoch 24/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2212 - accuracy: 0.9161 - val_loss: 0.2111 - val_accuracy: 0.9206\n",
            "Epoch 25/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2309 - accuracy: 0.9132 - val_loss: 0.2063 - val_accuracy: 0.9222\n",
            "Epoch 26/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2219 - accuracy: 0.9160 - val_loss: 0.2034 - val_accuracy: 0.9264\n",
            "Epoch 27/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2242 - accuracy: 0.9168 - val_loss: 0.2044 - val_accuracy: 0.9222\n",
            "Epoch 28/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2212 - accuracy: 0.9153 - val_loss: 0.2084 - val_accuracy: 0.9228\n",
            "Epoch 29/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2221 - accuracy: 0.9171 - val_loss: 0.2054 - val_accuracy: 0.9244\n",
            "Epoch 30/50\n",
            "1719/1719 [==============================] - 60s 35ms/step - loss: 0.2171 - accuracy: 0.9188 - val_loss: 0.2044 - val_accuracy: 0.9264\n",
            "Epoch 31/50\n",
            "1719/1719 [==============================] - 61s 35ms/step - loss: 0.2154 - accuracy: 0.9188 - val_loss: 0.1988 - val_accuracy: 0.9274\n",
            "Epoch 32/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.2166 - accuracy: 0.9154 - val_loss: 0.2242 - val_accuracy: 0.9160\n",
            "Epoch 33/50\n",
            "1719/1719 [==============================] - 61s 36ms/step - loss: 0.2147 - accuracy: 0.9196 - val_loss: 0.2045 - val_accuracy: 0.9260\n",
            "Epoch 34/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.2164 - accuracy: 0.9172 - val_loss: 0.2130 - val_accuracy: 0.9204\n",
            "Epoch 35/50\n",
            "1719/1719 [==============================] - 62s 36ms/step - loss: 0.2230 - accuracy: 0.9170 - val_loss: 0.2003 - val_accuracy: 0.9270\n",
            "Epoch 36/50\n",
            "1719/1719 [==============================] - 61s 36ms/step - loss: 0.2065 - accuracy: 0.9242 - val_loss: 0.2005 - val_accuracy: 0.9250\n",
            "Epoch 37/50\n",
            " 502/1719 [=======>......................] - ETA: 42s - loss: 0.2125 - accuracy: 0.9196"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w33o-DQHSgQs"
      },
      "source": [
        "# plot of loss and accuracy history \n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.gca().set_ylim(0,1) #set the vertical range to [0-1]\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Fraction')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-CFCZcclol"
      },
      "source": [
        "## Model Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZJHjlWGkOU_"
      },
      "source": [
        "#generate image of model \n",
        "keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6URaXNX5fipV"
      },
      "source": [
        "## Predictions on Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKG5ai9CgA1k"
      },
      "source": [
        "y_pred  = model.predict(X_test).round(2) #this would give the probability of all classes\n",
        "y_pred[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfXwEq3uhbyF"
      },
      "source": [
        "y_pred =np.argmax(y_pred, axis=-1) # this would give classes with the highest probability\n",
        "y_pred[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjUEFaWjBvh"
      },
      "source": [
        "# To cross check with actual value of y\n",
        "y_test[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0yyISVvilJp"
      },
      "source": [
        "np.array(class_names)[y_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgEtk6c2T9-X"
      },
      "source": [
        "## Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5i5Ah_tiCHZ"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK84TmHoT9gi"
      },
      "source": [
        "# Test data accuracy\n",
        "_, acc = model.evaluate(X_test, y_test)\n",
        "print('Test Dataset Accuracy: %.3f' % (acc * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8TcNo3Rg_eK"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-xyj_arjDU"
      },
      "source": [
        "#  Define confusion matrix\n",
        "con_mat = confusion_matrix(y_test, y_pred)\n",
        "confu_mat = con_mat / con_mat.sum(axis=1)\n",
        "\n",
        "# Print Confusion matrix\n",
        "print(\"Confusion matrix: \\n\", con_mat)\n",
        "\n",
        "# Plot Confusion matrix\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.imshow(confu_mat, interpolation='nearest', cmap='viridis')\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jrKXmt6FHel"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q6iPQ_yHzn8"
      },
      "source": [
        "The matrix and classification report show the fraction of true predictions for each class. The prediction of the 'shirt' class is the least reliable of all the classes.\n",
        "While 'coats' and bags are predicted with greater accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws_SYriuoI5-"
      },
      "source": [
        "### Visualize accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSoNznZ5oDIC"
      },
      "source": [
        "# Visualise 20 random test images along with their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(X_test.shape[0], size=20, replace=False)):\n",
        "    ax = figure.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(X_test[index]), cmap='Greys')\n",
        "    predict_index = y_pred[index]\n",
        "    true_index = y_test[index]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"Ground truth: {}\\nPredicted: {}\".format(class_names[predict_index],\n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck5fd18rtxzF"
      },
      "source": [
        "## Predictions on Validation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e98p6HOJtxzR"
      },
      "source": [
        "y_pred_val  = model.predict(X_valid).round(2) #this would give the probability of all classes\n",
        "y_pred_val[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyuCbo1ftxzT"
      },
      "source": [
        "y_pred_val =np.argmax(y_pred_val, axis=-1) # this would give classes with the highest probability\n",
        "y_pred_val[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYyrI5_QtxzV"
      },
      "source": [
        "# To cross check with actual value of y\n",
        "y_valid[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzWIK7PtxzV"
      },
      "source": [
        "np.array(class_names)[y_pred_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZjuJQVrChe"
      },
      "source": [
        "## Evaluation on Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXdA-loGJCQN"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM5mSbEmJCQN"
      },
      "source": [
        "# Test data accuracy\n",
        "_, acc = model.evaluate(X_valid, y_valid)\n",
        "print('Validation Dataset Accuracy: %.3f' % (acc * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl9OMT1gJCQP"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhZDE5-1JCQQ"
      },
      "source": [
        "#  Define confusion matrix\n",
        "con_mat = confusion_matrix(y_valid, y_pred_val)\n",
        "confu_mat = con_mat / con_mat.sum(axis=1)\n",
        "\n",
        "# Print Confusion matrix\n",
        "print(\"Confusion matrix for validation data: \\n\", con_mat)\n",
        "\n",
        "# Plot Confusion matrix\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.imshow(confu_mat, interpolation='nearest', cmap='viridis')\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhvs7oN3JCQS"
      },
      "source": [
        "print(classification_report(y_valid, y_pred_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoeCIqdsJCQS"
      },
      "source": [
        "The results of the matrix and classification report show the same trend as with the test data. The prediction of the 'shirt' class is the least reliable of all the classes.\n",
        "While 'coats' and bags are predicted with greater accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODQJ3X95rCh7"
      },
      "source": [
        "### Visualize accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5TJIzqwrCh7"
      },
      "source": [
        "# Visualise 20 random test images along with their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(X_valid.shape[0], size=20, replace=False)):\n",
        "    ax = figure.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(X_test[index]), cmap='Greys')\n",
        "    predict_index = y_pred[index]\n",
        "    true_index = y_test[index]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"Ground truth: {}\\nPredicted: {}\".format(class_names[predict_index],\n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}